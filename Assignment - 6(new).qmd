---
title: "Assignment - 6"
authors: 
  Maksuda Aktar Toma,
  Aarif Baksh
date: today
date-format: long
execute: 
  echo: false
  warning: false
columns: 2
format: html
editor: visual
---

## Business Problem

As an employee of CloverShield Insurance company, you are tasked with addressing the challenge of reducing call center costs. Your business partners have requested the development of a predictive model that, based on the provided segmentation, forecasts the number of times a policyholder is likely to call. This model aims to optimize resource allocation and enhance cost-efficiency in call center operations.

To find all our works on this project go to this link <https://github.com/maksudatoma/2024-Travelers-University-Modeling-Competition/tree/main>

## Introduction

The data obtained from Kaggle, is split into two parts: training data and validation data. In the validation data, the target variable, call_counts, is omitted. The training dataset contains 80,000 samples, and the validation dataset contains 20,000 samples.

**Variable Descriptions**

-   `ann_prm_amt`: Annualized Premium Amount

-   `bi_limit_group`: Body injury limit group (SP stands for single split limit coverage, CSL stands for combined single limit coverage)

-   `channel`: Distribution channel

-   `newest_veh_age`: The age of the newest vehicle insured on a policy (-20 represents non-auto or missing values)

-   `geo_group`: Indicates if the policyholder lives in a rural, urban, or suburban area

-   `has_prior_carrier`: Did the policyholder come from another carrier

-   `home_lot_sq_footage`: Square footage of the policyholder's home lot

-   `household_group`: The types of policy in household

-   `household_policy_counts`: Number of policies in the household

-   `telematics_ind`: Telematic indicator (0 represents auto missing values or didn't enroll and -2 represents non-auto)

-   `digital_contacts_ind`: An indicator to denote if the policy holder has opted into digital communication

-   `12m_call_history`: Past one year call count

-   `tenure_at_snapshot`:Policy active length in month

-   `pay_type_code`: Code indicating the payment method

-   `acq_method`:The acquisition method (Miss represents missing values)

-   `trm_len_mo`: Term length month

-   `pol_edeliv_ind`: An indicator for email delivery of documents (-2 represents missing values)

-   `aproduct_sbtyp_grp`: Product subtype group

-   `product_sbtyp`: Product subtype

-   `call_counts`: The number of call count generated by each policy (target variable)

## Data Cleaning and Missing Value count

First, we prepares the data by cleaning and transforming it (e.g., converting characters to factors, marking missing values.)

| Variable       | Number of missing values |
|----------------|--------------------------|
| acq_method     | 16,066                   |
| newest_veh_age | 58,015                   |
| pol_edeliv_ind | 838                      |
| telematics_ind | 58,015                   |

: **Table 1: Variables with Missing Values**

**Zero Values:** 50.18% of the rows in the call_counts column are zeros, indicating that most customers made no calls. This is significant and might suggest using models like Zero-Inflated Poisson (ZIP) to handle the high frequency of zeros.

**Key Takeaways** - The dataset contains both numeric and categorical variables, with some columns having significant missing values. - The target variable (call_counts) is heavily zero-inflated and skewed, which may require specialized modeling approaches. - Some numeric variables, like ann_prm_amt and home_lot_sq_footage, have wide ranges and outliers, suggesting that data transformation or scaling may be beneficial.

```{r}
library(caret)
library(dplyr)
library(mice)


trav <- read.csv("train_data.csv")

#Exclude first column (ID column)
trav <- trav[,-1]

trav <- trav %>%
  mutate(across(where(is.character), as.factor))

trav[trav == -2 |trav == -20 | trav == "missing"] <- NA

missing_counts <- colSums(is.na(trav))

# Display variables with missing values and their counts
missing_counts[missing_counts > 0]

#Zero values for the response
per0resp <- sum(trav$call_counts == 0) / nrow(trav) * 100
per0resp

```

```{r}
# Plot histogram for call_counts
hist(trav$call_counts,
     breaks = 30,  # Number of bins
     col = "blue",  # Fill color
     border = "black",  # Border color
     main = "Histogram of Call Counts",  # Title
     xlab = "Call Counts",  # X-axis label
     ylab = "Frequency",  # Y-axis label
     cex.main = 1.5,  # Text size for title
     cex.lab = 1.2,  # Text size for labels
     cex.axis = 1.2)  # Text size for axis

```

## Missing Value display

1.  This UpSet Plot shows the patterns and extent of missing data across variables. The horizontal bars on the left represent the total missing values for each variable, with newest_veh_age and telematics_ind having the most missing data. The vertical bars represent the number of rows with specific missingness patterns, with the tallest bar (\~45,731 rows) indicating that only newest_veh_age has missing values. The connected dots below highlight combinations of missingness across variables, with fewer rows having simultaneous missing values in multiple variables. This analysis suggests focusing on simple imputation for variables with isolated missingness and predictive modeling for overlapping patterns

```{r}
#Visualising pattern of missingness

#1. Heatmap
library(naniar)

# Visualize missing data with a heatmap
gg_miss_upset(trav)  # Upset plot to show combinations of missingness

```

2.  This chart shows the percentage and count of missing values for each feature in the dataset. Most features (green bars) have no missing data, making them ready for modeling. However, newest_veh_age and telematics_ind have significant missingness (72.52%), requiring advanced imputation or removal. acq_method has moderate missingness (20.08%), which can be addressed with simpler imputation methods. Features like pol_edeliv_ind (1.05% missing) require minimal effort to handle, such as mean or mode imputation. The focus should be on addressing features with high and moderate missingness to ensure data quality for modeling.

```{r}
#2.
library(DataExplorer)

# Visualize missing data
plot_missing(trav)
```

3.This visualization highlights missing data patterns in the dataset. The left panel shows that telematics_ind and newest_veh_age have the highest proportion of missing values (\~70%), while pol_edeliv_ind has a smaller proportion (\~10%). The right panel reveals that most rows have no missing data (blue squares), but missingness in telematics_ind and newest_veh_age often co-occurs. Other features have negligible or no missing data. It is recommended to either impute or exclude telematics_ind and newest_veh_age depending on their importance, while simpler imputation methods can handle pol_edeliv_ind.

```{r}
# 3. Heatmap )

library(VIM)

# Visualize missing data with a matrix plot
aggr(trav, col = c("skyblue", "red"), numbers = TRUE, sortVars = TRUE, 
     labels = names(trav), cex.axis = 0.7, gap = 3, ylab = c("Missing Data", "Pattern"))

```

## Correlation Structure for Call Count and Numeric Predictors

The correlations output show that X12m_call_history (r=0.28) is the strongest numeric predictor of call_counts, with a moderate positive relationship. Other variables like telematics_ind (ð‘Ÿ=0.0059) and pol_edeliv_ind (ð‘Ÿ=0.0049) have very weak positive correlations, while variables like household_policy_counts (r=âˆ’0.0033) and newest_veh_age (r=âˆ’0.0030) have negligible negative correlations. Most numeric variables show correlations close to zero, suggesting little to no linear relationship with the target variable. Overall, X12m_call_history is the most promising numeric predictor, while others may require further evaluation for relevance in modeling.

```{r}
#| echo: false
#Correlation for numeric predictors
num_vars <- sapply(trav, is.numeric)
correls <- sapply(trav[, num_vars], function(x) cor(x, trav$call_counts, use = "complete.obs"))

# Print correlations
print(correls)
```

**Correlation Matrix:** The correlation heatmap shows that X12m_call_history has the strongest positive correlation (râ‰ˆ0.28) with call_counts, making it the most important numeric predictor. Most other variables, such as ann_prm_amt, household_policy_counts, and home_lot_sq_footage, have weak or no significant correlations with the target variable, as indicated by grey cells. There are no strong negative correlations in the dataset. Overall, the relationships are mostly weak, suggesting that non-linear models or feature engineering may be needed to capture more complex interactions. The heatmap helps identify X12m_call_history as a key feature while others may contribute less linearly.

```{r}
# Load necessary library
library(ggplot2)
library(reshape2)

# Select numeric columns for correlation
num_vars <- sapply(trav, is.numeric)
correlation_matrix <- cor(trav[, num_vars], use = "complete.obs")

# Melt the correlation matrix for ggplot2
melted_corr <- melt(correlation_matrix)

# Plot the heatmap
ggplot(data = melted_corr, aes(x = Var1, y = Var2, fill = value)) +
  geom_tile(color = "white") +
  scale_fill_gradient2(low = "blue", high = "red", mid = "white", 
                       midpoint = 0, limit = c(-1, 1), space = "Lab", 
                       name = "Correlation") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(title = "Correlation Heatmap", x = "", y = "")

```

## Imputing Missing Values

The dataset is prepared by converting character columns to factors and handling missing data by replacing coded values such as `-2`, `-20`, and "missing" with `NA`. The code then calculates the percentage of zero values in the `call_counts` column to assess the distribution of the response variable. To address these missing values, the `mice` function performs multiple imputation, generating five potential datasets and selecting one for subsequent analysis to ensure consistency. For imputation, the `mice` function is used with a vector of default methods tailored to different types of variables: predictive mean matching (`pmm`) for numeric data, logistic regression imputation (`logreg`) for binary (factor with 2 levels), polytomous regression imputation (`polyreg`) for unordered categorical data with more than two levels, and the proportional odds model (`polr`) for ordered factors with more than two levels.

Finally, adjustments are made to factor variables to exclude "missing" as a level, preserving data integrity.

## ANOVA for relationship between Call Count and Categorical Predictors

The ANOVA results evaluate the effect of categorical variables on call_counts. Among the predictors, acq_method is marginally significant (p=0.0518), suggesting it may have a weak influence on call_counts. All other categorical variables, such as bi_limit_group, channel, and geo_group, have p-values greater than 0.1, indicating no statistically significant relationship with the target variable. Additionally, 16,066 rows were excluded due to missing data, which might affect the robustness of the results. It is recommended to focus on acq_method for further analysis and consider handling missing data to improve model accuracy.

```{r}
#ANOVA for categorical predictors
cat_vars <- sapply(trav, is.factor)

anova_results <- lapply(names(trav)[cat_vars], function(var) {
  anova_model <- aov(trav$call_counts ~ trav[[var]])
  summary(anova_model)
})

# Print ANOVA summaries
names(anova_results) <- names(trav)[cat_vars]
anova_results
```

**Call_counts distribution with significant predictor**
The violin plot shows the distribution of call_counts across different acquisition methods (acq_method). All methods have a heavily skewed distribution, with most values near 0 and a few extreme outliers, indicating that the majority of customers make few calls. The distributions are nearly identical across all methods, including the NA category, suggesting that acq_method has minimal impact on call_counts. This aligns with the ANOVA results, where acq_method was marginally significant. Further analysis, such as handling outliers or exploring interactions with other variables, may provide additional insights.

```{r}

# Create violin plot
ggplot(trav, aes(x = acq_method, y = call_counts)) +
  geom_violin(fill = "lightpink", trim = FALSE) +
  labs(title = "Distribution of Call Counts by Acquisition Method",
       x = "Acquisition Method",
       y = "Call Counts") +
  theme_minimal()


```

## Splitting Dataset

Before fitting any models, we will split the provided training dataset into three subsets: 60% for training, 20% for validation, and 20% for testing. This split will be done while ensuring stratification based on the variable call_counts. Stratification preserves the distribution of call_counts across all subsets, confirmed by the nearly identical means of the subsets. The training set is used to build the model, the validation set is used for tuning and performance assessment during training, and the test set is reserved for final evaluation. This ensures unbiased and representative splits for reliable model training and testing.

```{r}
# Split data into training set (60%)
set.seed(123)  # For reproducibility
trainIndex <- createDataPartition(trav$call_counts, p = 0.6, list = FALSE)
train_data <- trav[trainIndex, ]

# Remaining data (40%) for validation and test sets
rem_data <- trav[-trainIndex, ]

# Split remaining data into validation (20%) and test (20%)
validIndex <- createDataPartition(rem_data$call_counts, p = 0.5, list = FALSE)
valid_data <- rem_data[validIndex, ]
test_data <- rem_data[-validIndex, ]

# Checking whether stratification was successful
mean(train_data$call_counts)
mean(valid_data$call_counts)
mean(test_data$call_counts)
```

## Models

**Model 1 GBM** **:** We trained a GBM model using 500 trees with a Poisson distribution to predict call_counts. The hyperparameters for this model were selected by trial and error. Attempts to use specific functions (e.g. the train function) for hyperparameter tuning were unsuccessful due to insufficient computer memory needed to execute the code. This model achieved a test RMSE of 36.0051, indicating moderate prediction error, suggesting the predictions deviate by about 36 calls on average from actual values.

The variable importance plot shows the relative importance of the top 10 predictors. `X_12m_call_history` is the most important predictor, with its relative information gain being 92.64%. The remaining 9 variables account for only 7.38% of the relative information gain.

```{r}
#Model 1: GBM Model
library(gbm)
gbm.poisson <- gbm(call_counts ~ ., data = train_data, distribution="poisson", n.tree = 500, interaction.depth=7, shrinkage=0.01,n.minobsinnode=20,bag.fraction=1)


gbm.poissonpred <- predict(gbm.poisson, newdata = test_data, type = "response")
# Evaluate model performance using RMSE
rmse1 <- sqrt(mean((gbm.poissonpred - test_data$call_counts)^2))
rmse1

# Display variable importance and plot
summary(gbm.poisson)

var_importance <- summary(gbm.poisson, plotit = FALSE)

# Print variable importance data frame
print(var_importance)

# Plot variable importance manually
barplot(var_importance$rel.inf, names.arg = var_importance$var, las = 2, col = "lightblue", main = "Variable Importance")


```

**Model 2 ZIP:** The Zero-Inflated Poisson (ZIP) model predicts call_counts while accounting for excess zeros. An attempt to use all variable sin this model resulted in errors in convergence and `NA's` for the standard errors, z-value and p-value. Instead, a subset of variables was considered based on the variable importance plot from the GBM model.

The ZIP model has has two parts:

**The Count Model:** variables that directly affect the frequency of calls were considered. These include `X12_m_call_history`, `bi_limit_group` , `acq_method` , `geo_group` , and interaction between `acq_method` and `geo_group` , and between `X12_m_call_history` and `bi_limit_group`.

**The Zero-inflation Model:** variables that indicate whether a customer is likely to have any calls at all were considered. These include `X12_m_call_history` and `ann_prm_amt`. The test RMSE from this model is 36.5291, which is marginally poorer than the RSME from the GBM model.

```{r, echo=FALSE}

#Model 2: ZIP 

library(pscl)

# Zero-Inflated Poisson model

zip_model <- zeroinfl(call_counts ~ 
                        X12m_call_history + bi_limit_group + acq_method + geo_group
                        + acq_method*geo_group + bi_limit_group*X12m_call_history |
                        X12m_call_history + ann_prm_amt, data = train_data, dist = 
                        "poisson")

summary(zip_model)

zip_preds <- predict(zip_model, newdata = test_data, type = "response")
rmse2 <- sqrt(mean((zip_preds - test_data$call_counts)^2))
rmse2

```

## Appendix- R Code

```{r, , fig.pos="H"}
#| label: appendix A
#| echo: true
#| eval: false
library(caret)
library(dplyr)
library(mice)


trav <- read.csv("train_data.csv")

#Exclude first column (ID column)
trav <- trav[,-1]

trav <- trav %>%
  mutate(across(where(is.character), as.factor))

trav[trav == -2 |trav == -20 | trav == "missing"] <- NA

missing_counts <- colSums(is.na(trav))

# Display variables with missing values and their counts
missing_counts[missing_counts > 0]

#Zero values for the response
per0resp <- sum(trav$call_counts == 0) / nrow(trav) * 100
per0resp

```

```{r, , fig.pos="H"}
#| label: appendix A
#| echo: true
#| eval: false
# Plot histogram for call_counts
hist(trav$call_counts,
     breaks = 30,  # Number of bins
     col = "blue",  # Fill color
     border = "black",  # Border color
     main = "Histogram of Call Counts",  # Title
     xlab = "Call Counts",  # X-axis label
     ylab = "Frequency",  # Y-axis label
     cex.main = 1.5,  # Text size for title
     cex.lab = 1.2,  # Text size for labels
     cex.axis = 1.2)  # Text size for axis

```


```{r, , fig.pos="H"}
#| label: appendix A
#| echo: true
#| eval: false
#Visualising pattern of missingness

#1. Heatmap
library(naniar)

# Visualize missing data with a heatmap
gg_miss_upset(trav)  # Upset plot to show combinations of missingness

#2.
library(DataExplorer)

# Visualize missing data
plot_missing(trav)

# 3. Heatmap )

library(VIM)

# Visualize missing data with a matrix plot
aggr(trav, col = c("skyblue", "red"), numbers = TRUE, sortVars = TRUE, 
     labels = names(trav), cex.axis = 0.7, gap = 3, ylab = c("Missing Data", "Pattern"))
```


```{r, , fig.pos="H"}
#| label: appendix A
#| echo: true
#| eval: false
# Load necessary library
library(ggplot2)
library(reshape2)

# Select numeric columns for correlation
num_vars <- sapply(trav, is.numeric)
correlation_matrix <- cor(trav[, num_vars], use = "complete.obs")

# Melt the correlation matrix for ggplot2
melted_corr <- melt(correlation_matrix)

# Plot the heatmap
ggplot(data = melted_corr, aes(x = Var1, y = Var2, fill = value)) +
  geom_tile(color = "white") +
  scale_fill_gradient2(low = "blue", high = "red", mid = "white", 
                       midpoint = 0, limit = c(-1, 1), space = "Lab", 
                       name = "Correlation") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(title = "Correlation Heatmap", x = "", y = "")

```

```{r, , fig.pos="H"}
#| label: appendix A
#| echo: true
#| eval: false
#ANOVA for categorical predictors
cat_vars <- sapply(trav, is.factor)

anova_results <- lapply(names(trav)[cat_vars], function(var) {
  anova_model <- aov(trav$call_counts ~ trav[[var]])
  summary(anova_model)
})

# Print ANOVA summaries
names(anova_results) <- names(trav)[cat_vars]
anova_results
```

```{r, , fig.pos="H"}
#| label: appendix A
#| echo: true
#| eval: false
# Create violin plot
ggplot(trav, aes(x = acq_method, y = call_counts)) +
  geom_violin(fill = "lightpink", trim = FALSE) +
  labs(title = "Distribution of Call Counts by Acquisition Method",
       x = "Acquisition Method",
       y = "Call Counts") +
  theme_minimal()
```

```{r, , fig.pos="H"}
#| label: appendix A
#| echo: true
#| eval: false
# Split data into training set (60%)
set.seed(123)  # For reproducibility
trainIndex <- createDataPartition(trav$call_counts, p = 0.6, list = FALSE)
train_data <- trav[trainIndex, ]

# Remaining data (40%) for validation and test sets
rem_data <- trav[-trainIndex, ]

# Split remaining data into validation (20%) and test (20%)
validIndex <- createDataPartition(rem_data$call_counts, p = 0.5, list = FALSE)
valid_data <- rem_data[validIndex, ]
test_data <- rem_data[-validIndex, ]

# Checking whether stratification was successful
mean(train_data$call_counts)
mean(valid_data$call_counts)
mean(test_data$call_counts)
```

```{r, , fig.pos="H"}
#| label: appendix A
#| echo: true
#| eval: false
#Model 1: GBM Model
library(gbm)
gbm.poisson <- gbm(call_counts ~ ., data = train_data, distribution="poisson", n.tree = 500, interaction.depth=7, shrinkage=0.01,n.minobsinnode=20,bag.fraction=1)


gbm.poissonpred <- predict(gbm.poisson, newdata = test_data, type = "response")
# Evaluate model performance using RMSE
rmse1 <- sqrt(mean((gbm.poissonpred - test_data$call_counts)^2))
rmse1

# Display variable importance and plot
summary(gbm.poisson)

var_importance <- summary(gbm.poisson, plotit = FALSE)

# Print variable importance data frame
print(var_importance)

# Plot variable importance manually
barplot(var_importance$rel.inf, names.arg = var_importance$var, las = 2, col = "lightblue", main = "Variable Importance")

```

```{r, , fig.pos="H"}
#| label: appendix A
#| echo: true
#| eval: false

#Model 2: ZIP 

library(pscl)

# Zero-Inflated Poisson model

zip_model <- zeroinfl(call_counts ~ 
                        X12m_call_history + bi_limit_group + acq_method + geo_group
                        + acq_method*geo_group + bi_limit_group*X12m_call_history |
                        X12m_call_history + ann_prm_amt, data = train_data, dist = 
                        "poisson")

summary(zip_model)

zip_preds <- predict(zip_model, newdata = test_data, type = "response")
rmse2 <- sqrt(mean((zip_preds - test_data$call_counts)^2))
rmse2
```