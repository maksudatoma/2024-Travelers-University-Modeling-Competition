---
title: "Final Report"
format:
  pdf:
    keep-tex: true
    documentclass: article
    papersize: letter
    fontsize: 10pt
    template-partials:
      - title.tex
      - before-bib.tex
    include-in-header:
      - text: |
          \usepackage{sdss2020} % Uses Times Roman font (either newtx or times package)
          \usepackage{url}
          \usepackage{hyperref}
          \usepackage{latexsym}
          \usepackage{amsmath, amsthm, amsfonts}
          \usepackage{algorithm, algorithmic}
          \usepackage[dvipsnames]{xcolor} % colors
          \newcommand{\mt}[1]{{\textcolor{blue}{#1}}}
          \newcommand{\svp}[1]{{\textcolor{RedOrange}{#1}}}
    classoption: [twocolumn]
    mainfont: Times New Roman
    colorlinks: true

author:
  - name: "Maksuda Aktar Toma"
    affiliation: "Statistics Department, University of Nebraska, Lincoln"
    email: "mtoma2@huskers.unl.edu"
    corresponding: true
  
  - name: "Aarif Baksh"
    affiliation: "Statistics Department, University of Nebraska, Lincoln"
    email: "abaksh2@unl.edu"
    corresponding: true
    orcid: "0000-0002-3803-0972"
bibliographystyle: acl
bibliography: refs.bib
filters:
  - latex-environment
commands: [mt, svp]
---

## Business Problem

As an employee of CloverShield Insurance company, you are tasked with addressing the challenge of reducing call center costs. Your business partners have requested the development of a predictive model that, based on the provided segmentation, forecasts the number of times a policyholder is likely to call. This model aims to optimize resource allocation and enhance cost-efficiency in call center operations.

To find all our works on this project go to this link <https://github.com/maksudatoma/2024-Travelers-University-Modeling-Competition/tree/main>

## Introduction

The data obtained from Kaggle, is split into two parts: training data and validation data. In the validation data, the target variable, call_counts, is omitted. The training dataset contains 80,000 samples, and the validation dataset contains 20,000 samples.There are several variables where "call_counts` (The number of call count generated by each policy) is the target variable. The other variables that were used as predicted variables are  `ann_prm_amt`(Annualized Premium Amount), `bi_limit_group` (Body injury limit group),`channel` (Distribution channel), `newest_veh_age` (The age of the newest vehicle insured on a policy (-20 represents non-auto or missing values)),`geo_group`(Indicates if the policyholder lives in a rural, urban, or suburban area),`has_prior_carrier` (Did the policyholder come from another carrier),`home_lot_sq_footage` (Square footage of the policyholder's home lot),`household_group` (The types of policy in household),`household_policy_counts` (Number of policies in the household),`telematics_ind` (Telematic indicator (0 represents auto missing values or didn't enroll and -2 represents non-auto)),`digital_contacts_ind` (An indicator to denote if the policy holder has opted into digital communication),`12m_call_history`(Past one year call count),`tenure_at_snapshot`(Policy active length in month),`pay_type_code`(Code indicating the payment method),`acq_method`(The acquisition method (Miss represents missing values)),`trm_len_mo`(Term length month),`pol_edeliv_ind`(An indicator for email delivery of documents (-2 represents missing values)),`aproduct_sbtyp_grp`(Product subtype group),`product_sbtyp` (Product subtype)


## Methodology

**Imputation**

Missing values in the dataset were imputed using the *Multivariate Imputation by Chained Equations (MICE)* package in R. MICE generates "plausible" synthetic values for incomplete columns based on the relationships with other variables in the dataset. The imputation process uses a Markov Chain Monte Carlo (MCMC) approach, specifically a technique known as *Gibbs sampling*, which iteratively updates missing values by sampling from conditional distributions of the observed data.

In this dataset, *four variables* contain missing values: `acq_method`(20%), `newest_veh_age`(72%), `pol_edeliv_ind`(1%), and `telematics_ind`(72%). Each variable was imputed using methods appropriate for its type:

1.  **`acq_method`**: A **nominal variable** with four categories. Missing values were imputed using **polytomous logistic regression** (`polyreg`), which is designed for categorical variables with more than two levels.
2.  **`newest_veh_age`**: A *numeric variable*. Missing values were imputed using **Predictive Mean Matching** (`pmm`), which ensures imputed values are plausible by selecting observed values close to the predicted mean.
3.  **`pol_edeliv_ind`** and **`telematics_ind`**: Both are **binary variables**. Missing values were imputed using **logistic regression** (`logreg`), which models binary outcomes effectively.

**Zero Values:** 
50.18% of the rows in the call_counts column are zeros, indicating that most customers made no calls. This is significant and might suggest using models like Zero-Inflated Poisson (ZIP) to handle the high frequency of zeros.

Overall, The dataset includes both categorical and numerical variables, and there are notable missing values in a few of the columns. The target variable, call_counts, is highly skewed and zero-inflated, necessitating the use of specific modeling techniques. Some numerical variables, such as home_lot_sq_footage and ann_prm_amt, contain large ranges and outliers, indicating that scaling or data transformation would be helpful.


![Heat Map](000012.png){width="50%"}

The correlation heatmap shows that X12m_call_history has the strongest positive correlation (râ‰ˆ0.28) with call_counts, making it the most important numeric predictor. Most other variables, such as ann_prm_amt, household_policy_counts, and home_lot_sq_footage, have weak or no significant correlations with the target variable, as indicated by grey cells. There are no strong negative correlations in the dataset. Overall, the relationships are mostly weak, suggesting that non-linear models or feature engineering may be needed to capture more complex interactions. The heatmap helps identify X12m_call_history as a key feature while others may contribute less linearly.

# Result

**ANOVA Table**

The ANOVA results evaluate the effect of categorical variables on call_counts. Among the predictors, acq_method is marginally significant (p=0.0518), suggesting it may have a weak influence on call_counts. All other categorical variables, such as bi_limit_group, channel, and geo_group, have p-values greater than 0.1, indicating no statistically significant relationship with the target variable. Additionally, 16,066 rows were excluded due to missing data, which might affect the robustness of the results. It is recommended to focus on acq_method for further analysis and consider handling missing data to improve model accuracy.

![ANOVA result](anova_results.jpg){width="50%"}

The violin plot shows the distribution of call_counts across different acquisition methods (acq_method). All methods have a heavily skewed distribution, with most values near 0 and a few extreme outliers, indicating that the majority of customers make few calls. The distributions are nearly identical across all methods, including the NA category, suggesting that acq_method has minimal impact on call_counts. This aligns with the ANOVA results, where acq_method was marginally significant. Further analysis, such as handling outliers or exploring interactions with other variables, may provide additional insights.

![*Violin Plot*](17.png){width="50%"}

## Model Result

The Gradient Boosted Machine (GBM) attained the lowest RMSE of 36.1614, signifying superior predictive accuracy compared to other evaluated models, with Random Forest closely trailing at an RMSE of 36.30212. The Zero-Inflated Poisson (ZIP) and Zero-Inflated Negative Binomial (ZINB) models exhibited elevated RMSE values, signifying diminished accuracy. The Hurdle and Two-Part Models were contemplated but remain untested, allowing for future assessment. Gradient Boosting Machine (GBM) and Random Forest have the highest performance according to Root Mean Square Error (RMSE). Additional evaluation of the Hurdle and Two-Part Models may yield chances for enhancing forecasts.
![Model Comparison](model_results.jpg){width="50%"}

**I HAVE STOPPED HERE. CAN YOU WRITE NEXT PROCEDURE IN DETAILS?**
**NEED TO FOCUS ON THESE**
3. In the Method section describe the technical details of the steps you had taken. techincal description of imputation. If you are using GLM, what are the models for Bernoulli section and the Count section. If you are using RF, what is the node cost function, stopping rule, etc.

4. In the result section offer all model comparison result. Describe if you were doing a full cross-validation or a single hold out test. if you are using single hold out, why is that appropriate?

5. How you are you handling multiple tuning parameters that you obtain in each fold of CV?

## Model Selection

**Gradient Boosting Machine (GBM)**

-   Test RMSE: 36.1614

-   Best Performing Model

-   **Parameter Tuning**: Trial and Error

-   **Challenge**: Dataset was too large for hyperparameter tuning

## Variable Selection

**Gradient Boosting Machine (GBM)**
![Variable Importance Plot](VariableImportance.png){width="80%"}

An initial GBM was run with all the variables, and then a subset of 3 variables was selected from the variable importance plot, and another gbm model was run with those three variables.

## Variable Selection

![Important Variable](var_imp_val.png){width="70%"}

-   Most Important Variables: X12m_call_history, tenure_at_snapshot, and acq_method
-   Test RMSE for Model with all variables: 36.1742
-   Test RMSE for Model with 3 variables selected from Variable Importance Plot: 36.1614
-   Limitation: Variable importance does not specify the relationship between the predictors and call_counts

## Model Evaluation
![Train and Test RMSE Curves](Test_Train.png){width="70%"}

-   Train RMSE: 35.67179
-   Test RMSE: 36.1742

## Concerns

The model is likely sub-optimal, as it struggled to achieve a good accuracy score (about 25% on the validation set) and the parameters were tuned through trial and error instead of using a grid search to find the optimal values.

## Recommendations

With better computing power, implementing a grid search would be feasible and could significantly enhance the model's predictive capability.

# References

## Data Preprocessing

### Recoding



## Exploratory Data Analysis

## Results
