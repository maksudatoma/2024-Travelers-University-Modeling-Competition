---
title: "Assignment - 6"
authors: 
  Maksuda Aktar Toma,
  Aarif Baksh
date: today
date-format: long
execute: 
  echo: false
  warning: false
columns: 2
format: html
editor: visual
---

## Introduction

The data is split into two parts: training data and validation data. In the validation data, the target variable, call_counts, is omitted.

**Variable Descriptions**

-   ann_prm_amt: Annualized Premium Amount

-   bi_limit_group: Body injury limit group (SP stands for single split limit coverage, CSL stands for combined single limit coverage)

-   channel: Distribution channel

-   newest_veh_age: The age of the newest vehicle insured on a policy (-20 represents non-auto or missing values)

-   geo_group: Indicates if the policyholder lives in a rural, urban, or suburban area

-   has_prior_carrier: Did the policyholder come from another carrier

-   home_lot_sq_footage: Square footage of the policyholder's home lot

-   household_group: The types of policy in household

-   household_policy_counts: Number of policies in the household

-   telematics_ind: Telematic indicator (0 represents auto missing values or didn't enroll and -2 represents non-auto)

-   digital_contacts_ind: An indicator to denote if the policy holder has opted into digital communication

-   12m_call_history: Past one year call count

-   tenure_at_snapshot: Policy active length in month

-   pay_type_code: Code indicating the payment method

-   acq_mthd: The acquisition method (Miss represents missing values)

-   trm_len_mo: Term length month

-   pol_edeliv_ind: An indicator for email delivery of documents (-2 represents missing values)

-   product_sbtyp_grp: Product subtype group

-   product_sbtyp: Product subtype

-   call_counts: The number of call count generated by each policy (target variable)

## Data Cleaning and Missing Value count

First, we prepares the data by cleaning and transforming it (e.g., converting characters to factors, marking missing values). Then, analyzes the extent of missing data, calculates the percentage of zeros in the response variable (call_counts), which is important for modeling or imputation strategies.

**These variables show the number of missing values:**
acq_method: 16,066 missing values.
newest_veh_age: 58,015 missing values.
pol_edeliv_ind: 838 missing values.
telematics_ind: 58,015 missing values

**Zero Values:** 50.18% of the rows in the call_counts column are zeros, indicating that most customers made no calls. This is significant and might suggest using models like Zero-Inflated Poisson (ZIP) to handle the high frequency of zeros.

**Key Takeaways**
 - The dataset contains both numeric and categorical variables, with some columns having significant missing values.
 - The target variable (call_counts) is heavily zero-inflated and skewed, which may require specialized modeling approaches.
 - Some numeric variables, like ann_prm_amt and home_lot_sq_footage, have wide ranges and outliers, suggesting that data transformation or scaling may be beneficial.
```{r}
library(caret)
library(dplyr)
library(mice)
trav <- read.csv("train_data.csv")

#Exclude first column (ID column)
trav <- trav[,-1]

trav <- trav %>%
  mutate(across(where(is.character), as.factor))

trav[trav == -2 |trav == -20 | trav == "missing"] <- NA

missing_counts <- colSums(is.na(trav))

# Display variables with missing values and their counts
missing_counts[missing_counts > 0]

#Zero values for the response
per0resp <- sum(trav$call_counts == 0) / nrow(trav) * 100
per0resp

```
```{r}
# Plot histogram for call_counts
hist(trav$call_counts,
     breaks = 30,  # Number of bins
     col = "blue",  # Fill color
     border = "black",  # Border color
     main = "Histogram of Call Counts",  # Title
     xlab = "Call Counts",  # X-axis label
     ylab = "Frequency",  # Y-axis label
     cex.main = 1.5,  # Text size for title
     cex.lab = 1.2,  # Text size for labels
     cex.axis = 1.2)  # Text size for axis

```

## Missing Value display
1. This UpSet Plot shows the patterns and extent of missing data across variables. The horizontal bars on the left represent the total missing values for each variable, with newest_veh_age and telematics_ind having the most missing data. The vertical bars represent the number of rows with specific missingness patterns, with the tallest bar (~45,731 rows) indicating that only newest_veh_age has missing values. The connected dots below highlight combinations of missingness across variables, with fewer rows having simultaneous missing values in multiple variables. This analysis suggests focusing on simple imputation for variables with isolated missingness and predictive modeling for overlapping patterns
```{r}
#Visualising pattern of missingness

#1. Heatmap
library(naniar)

# Visualize missing data with a heatmap
gg_miss_upset(trav)  # Upset plot to show combinations of missingness

```
2. This chart shows the percentage and count of missing values for each feature in the dataset. Most features (green bars) have no missing data, making them ready for modeling. However, newest_veh_age and telematics_ind have significant missingness (72.52%), requiring advanced imputation or removal. acq_method has moderate missingness (20.08%), which can be addressed with simpler imputation methods. Features like pol_edeliv_ind (1.05% missing) require minimal effort to handle, such as mean or mode imputation. The focus should be on addressing features with high and moderate missingness to ensure data quality for modeling.
```{r}
#2.
library(DataExplorer)

# Visualize missing data
plot_missing(trav)
```
3.This visualization highlights missing data patterns in the dataset. The left panel shows that telematics_ind and newest_veh_age have the highest proportion of missing values (~70%), while pol_edeliv_ind has a smaller proportion (~10%). The right panel reveals that most rows have no missing data (blue squares), but missingness in telematics_ind and newest_veh_age often co-occurs. Other features have negligible or no missing data. It is recommended to either impute or exclude telematics_ind and newest_veh_age depending on their importance, while simpler imputation methods can handle pol_edeliv_ind.
```{r}
# 3. Heatmap )

library(VIM)

# Visualize missing data with a matrix plot
aggr(trav, col = c("skyblue", "red"), numbers = TRUE, sortVars = TRUE, 
     labels = names(trav), cex.axis = 0.7, gap = 3, ylab = c("Missing Data", "Pattern"))

```


## Correlation Structure

The correlations output show that X12m_call_history (r=0.28) is the strongest numeric predictor of call_counts, with a moderate positive relationship. Other variables like telematics_ind (𝑟=0.0059) and pol_edeliv_ind (𝑟=0.0049) have very weak positive correlations, while variables like household_policy_counts (r=−0.0033) and newest_veh_age (r=−0.0030) have negligible negative correlations. Most numeric variables show correlations close to zero, suggesting little to no linear relationship with the target variable. Overall, X12m_call_history is the most promising numeric predictor, while others may require further evaluation for relevance in modeling.

```{r}
#| echo: false
#Correlation for numeric predictors
num_vars <- sapply(trav, is.numeric)
correls <- sapply(trav[, num_vars], function(x) cor(x, trav$call_counts, use = "complete.obs"))

# Print correlations
print(correls)
```
**Correlation Matrix:**
The correlation heatmap shows that X12m_call_history has the strongest positive correlation (r≈0.28) with call_counts, making it the most important numeric predictor. Most other variables, such as ann_prm_amt, household_policy_counts, and home_lot_sq_footage, have weak or no significant correlations with the target variable, as indicated by grey cells. There are no strong negative correlations in the dataset. Overall, the relationships are mostly weak, suggesting that non-linear models or feature engineering may be needed to capture more complex interactions. The heatmap helps identify X12m_call_history as a key feature while others may contribute less linearly.


```{r}
# Load necessary library
library(ggplot2)
library(reshape2)

# Select numeric columns for correlation
num_vars <- sapply(trav, is.numeric)
correlation_matrix <- cor(trav[, num_vars], use = "complete.obs")

# Melt the correlation matrix for ggplot2
melted_corr <- melt(correlation_matrix)

# Plot the heatmap
ggplot(data = melted_corr, aes(x = Var1, y = Var2, fill = value)) +
  geom_tile(color = "white") +
  scale_fill_gradient2(low = "blue", high = "red", mid = "white", 
                       midpoint = 0, limit = c(-1, 1), space = "Lab", 
                       name = "Correlation") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(title = "Correlation Heatmap", x = "", y = "")

```

## Anova
The ANOVA results evaluate the effect of categorical variables on call_counts. Among the predictors, acq_method is marginally significant (p=0.0518), suggesting it may have a weak influence on call_counts. All other categorical variables, such as bi_limit_group, channel, and geo_group, have p-values greater than 0.1, indicating no statistically significant relationship with the target variable. Additionally, 16,066 rows were excluded due to missing data, which might affect the robustness of the results. It is recommended to focus on acq_method for further analysis and consider handling missing data to improve model accuracy.

```{r}
#ANOVA for categorical predictors
cat_vars <- sapply(trav, is.factor)

anova_results <- lapply(names(trav)[cat_vars], function(var) {
  anova_model <- aov(trav$call_counts ~ trav[[var]])
  summary(anova_model)
})

# Print ANOVA summaries
names(anova_results) <- names(trav)[cat_vars]
anova_results
```


**Call_counts distribution with significant predictor** [Note: Should keep only one]
The violin plot shows the distribution of call_counts across different acquisition methods (acq_method). All methods have a heavily skewed distribution, with most values near 0 and a few extreme outliers, indicating that the majority of customers make few calls. The distributions are nearly identical across all methods, including the NA category, suggesting that acq_method has minimal impact on call_counts. This aligns with the ANOVA results, where acq_method was marginally significant. Further analysis, such as handling outliers or exploring interactions with other variables, may provide additional insights.

```{r}
# Create jitter plot
ggplot(trav, aes(x = acq_method, y = call_counts)) +
  geom_jitter(width = 0.2, alpha = 0.3, color = "blue") +
  labs(title = "Call Counts by Acquisition Method",
       x = "Acquisition Method",
       y = "Call Counts") +
  theme_minimal()

# Create violin plot
ggplot(trav, aes(x = acq_method, y = call_counts)) +
  geom_violin(fill = "lightpink", trim = FALSE) +
  labs(title = "Distribution of Call Counts by Acquisition Method",
       x = "Acquisition Method",
       y = "Call Counts") +
  theme_minimal()

# Ensure 'acq_method' is a factor
trav$acq_method <- as.factor(trav$acq_method)

# Load ggplot2 for plotting
library(ggplot2)

# Create boxplot
ggplot(trav, aes(x = acq_method, y = call_counts)) +
  geom_boxplot(fill = "lightblue", outlier.color = "red", outlier.shape = 1) +
  labs(title = "Call Counts by Acquisition Method",
       x = "Acquisition Method",
       y = "Call Counts") +
  theme_minimal()


```

## Splitting Dataset
Before diiging with the model fit we'll split the data set.The dataset splits into training (60%), validation (20%), and test (20%) sets, ensuring stratification of the call_counts variable. Stratification preserves the distribution of call_counts across all subsets, confirmed by the nearly identical means of the subsets. The training set is used to build the model, the validation set is used for tuning and performance assessment during training, and the test set is reserved for final evaluation. This ensures unbiased and representative splits for reliable model training and testing.

```{r}
# Split data into training set (60%)
set.seed(123)  # For reproducibility
trainIndex <- createDataPartition(trav$call_counts, p = 0.6, list = FALSE)
train_data <- trav[trainIndex, ]

# Remaining data (40%) for validation and test sets
rem_data <- trav[-trainIndex, ]

# Split remaining data into validation (20%) and test (20%)
validIndex <- createDataPartition(rem_data$call_counts, p = 0.5, list = FALSE)
valid_data <- rem_data[validIndex, ]
test_data <- rem_data[-validIndex, ]

# Checking whether stratification was successful
mean(train_data$call_counts)
mean(valid_data$call_counts)
mean(test_data$call_counts)

```


## Model
**Model - 1:** GBM
We trained a GBM model using 500 trees with a Poisson distribution to predict call_counts. The model achieved an RMSE of 36.06, indicating moderate prediction error, suggesting the predictions deviate by about 36 calls on average from actual values. This result highlights potential room for improvement, such as hyperparameter tuning (e.g., adjusting n.tree, interaction.depth, or shrinkage), feature engineering, or handling outliers in the data. Comparing this RMSE with other models can help identify the best-performing approach. 

```{r}
#GBM Model
library(gbm)
gbm.poisson <- gbm(call_counts ~ ., data = train_data, distribution="poisson", n.tree = 500, interaction.depth=7, shrinkage=0.01,n.minobsinnode=20,bag.fraction=1)

gbm.poissonpred <- predict(gbm.poisson, newdata = test_data, type = "response")
# Evaluate model performance using RMSE
rmse8 <- sqrt(mean((gbm.poissonpred - test_data$call_counts)^2))
rmse8
```
**Model - 2:** ZIP
The Zero-Inflated Poisson (ZIP) model predicts call_counts while accounting for excess zeros. In the count model, X12m_call_history and prdct_sbtyp_grp significantly influence the count of call_counts, with higher X12m_call_history associated with increased counts. In the zero-inflation model, X12m_call_history reduces the likelihood of excess zeros, while prdct_sbtyp_grp has no significant effect on zero counts. The model effectively handles the over-representation of zeros and identifies X12m_call_history as a key predictor for both count and zero-inflation processes. Further evaluation against simpler models (e.g., Poisson) or adding more predictors may enhance its performance.

```{r-2, echo=FALSE}

#Method 2 - ZIP

library(pscl)

# Zero-Inflated Poisson model
zip_model <- zeroinfl(call_counts ~ X12m_call_history+prdct_sbtyp_grp, data = train_data, dist = "poisson")

summary(zip_model)

```
\newpage

# Appendix A - R Code
[Haven't put all codes]

```{r, , fig.pos="H"}
#| label: appendix A
#| echo: true
#| eval: false

library(caret)
library(dplyr)
library(mice)
trav <- read.csv("train_data.csv")

#Exclude first column (ID column)
trav <- trav[,-1]

trav <- trav %>%
  mutate(across(where(is.character), as.factor))

trav[trav == -2 |trav == -20 | trav == "missing"] <- NA

missing_counts <- colSums(is.na(trav))

# Display variables with missing values and their counts
missing_counts[missing_counts > 0]

#Zero values for the response
per0resp <- sum(trav$call_counts == 0) / nrow(trav) * 100
per0resp


```


